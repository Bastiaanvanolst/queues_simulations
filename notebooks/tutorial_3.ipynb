{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3, solutions\n",
    "\n",
    "\n",
    "This solution is a jupyter notebook which allows you to directly interact with the code so that\n",
    "you can see the effect of any changes you may like to make.\n",
    "\n",
    "Author: Nicky van Foreest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klara 18\n",
      "Piet 20\n",
      "Jan 21\n",
      "Cynthia 25\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappop, heappush\n",
    "import scipy\n",
    "from scipy.stats import uniform, expon\n",
    "\n",
    "scipy.random.seed(3)\n",
    "\n",
    "def sort_ages():\n",
    "    stack = []\n",
    "\n",
    "    heappush(stack, (21, \"Jan\"))\n",
    "    heappush(stack, (20, \"Piet\"))\n",
    "    heappush(stack, (18, \"Klara\"))\n",
    "    heappush(stack, (25, \"Cynthia\"))\n",
    "\n",
    "    while stack:\n",
    "        age, name = heappop(stack)\n",
    "        print(name, age)\n",
    "\n",
    "sort_ages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Klara Motorola\n",
      "20 Piet Apple\n",
      "21 Jan Huawei\n",
      "25 Cynthia Nexus\n"
     ]
    }
   ],
   "source": [
    "def sort_ages_with_more_info():\n",
    "    stack = []\n",
    "\n",
    "    heappush(stack, (21, \"Jan\", \"Huawei\"))\n",
    "    heappush(stack, (20, \"Piet\", \"Apple\"))\n",
    "    heappush(stack, (18, \"Klara\", \"Motorola\"))\n",
    "    heappush(stack, (25, \"Cynthia\", \"Nexus\"))\n",
    "\n",
    "    while stack:\n",
    "        age, name, phone = heappop(stack)\n",
    "        print(age, name, phone)\n",
    "\n",
    "sort_ages_with_more_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# void  code for numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARRIVAL = 0\n",
    "DEPARTURE = 1\n",
    "\n",
    "stack = [] # this is the event stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    def __init__(self):\n",
    "        self.arrival_time = 0\n",
    "        self.service_time = 0\n",
    "        self.departure_time = 0\n",
    "        self.queue_length_at_arrival = 0\n",
    "\n",
    "    def sojourn_time(self):\n",
    "        return self.departure_time - self.arrival_time\n",
    "\n",
    "    def waiting_time(self):\n",
    "        return self.sojourn_time() - self.service_time\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.arrival_time}, {self.service_time}, {self.departure_time}\\n\"\n",
    "\n",
    "    def __le__(self, other):\n",
    "        # this is necessary to sort jobs when they have the same arrival times.\n",
    "        return self.id <= other.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4001411934412399, 0.4105026155801149, 0\n",
      "\n",
      "0.5720238942868375, 0.2383467686762716, 0\n",
      "\n",
      "1.6892393010894828, 0.7553955070531961, 0\n",
      "\n",
      "1.756339572439428, 0.07741279395386719, 0\n",
      "\n",
      "1.7827590288232624, 0.19375523043743875, 0\n",
      "\n",
      "1.7979248278113555, 0.20344628960353223, 0\n",
      "\n",
      "2.321614593718849, 0.10880175821867327, 0\n",
      "\n",
      "2.885513997386044, 0.29790158981221126, 0\n",
      "\n",
      "2.8976510621876375, 0.272793197341875, 0\n",
      "\n",
      "3.04769876009649, 0.17877214435204763, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def experiment_1():\n",
    "    labda = 2.0\n",
    "    mu = 3.0\n",
    "    rho = labda / mu\n",
    "    F = expon(scale=1.0 / labda)  # interarrival time distributon\n",
    "    G = expon(scale=1.0 / mu)  # service time distributon\n",
    "\n",
    "    num_jobs = 10\n",
    "\n",
    "    time = 0\n",
    "    for i in range(num_jobs):\n",
    "        time += F.rvs()\n",
    "        job = Job()\n",
    "        job.arrival_time = time\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        print(job)\n",
    "\n",
    "\n",
    "\n",
    "experiment_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.busy = False\n",
    "\n",
    "server = Server()\n",
    "queue = []\n",
    "served_jobs = [] # used for statistics\n",
    "\n",
    "def start_service(time, job):\n",
    "    server.busy = True\n",
    "    job.departure_time = time + job.service_time\n",
    "    heappush(stack, (job.departure_time, job, DEPARTURE))\n",
    "\n",
    "def handle_arrival(time, job):\n",
    "    job.queue_length_at_arrival = len(queue)\n",
    "    if server.busy:\n",
    "        queue.append(job)\n",
    "    else:\n",
    "        start_service(time, job)\n",
    "\n",
    "def handle_departure(time, job):\n",
    "    server.busy = False\n",
    "    if queue: # queue is not empty\n",
    "        next_job = queue.pop(0) # pop oldest job in queue\n",
    "        start_service(time, next_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical avg. queue length:  1.333333333333333\n",
      "Simulated avg. queue length: 0.1\n",
      "Theoretical avg. sojourn time: 0.9999999999999999\n",
      "Simulated avg. sojourn time: 0.43616282815102236\n"
     ]
    }
   ],
   "source": [
    "def experiment_2():\n",
    "    labda = 2.0\n",
    "    mu = 3.0\n",
    "    rho = labda / mu\n",
    "    F = expon(scale=1.0 / labda)  # interarrival time distributon\n",
    "    G = expon(scale=1.0 / mu)  # service time distributon\n",
    "    num_jobs = 10  # too small, change it to a larger number, and rerun the experiment\n",
    "\n",
    "    time = 0\n",
    "    for i in range(num_jobs):\n",
    "        time += F.rvs()\n",
    "        job = Job()\n",
    "        job.arrival_time = time\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)\n",
    "    av_queue_length = tot_queue / len(served_jobs)\n",
    "    print(\"Theoretical avg. queue length: \", rho * rho / (1 - rho))\n",
    "    print(\"Simulated avg. queue length:\", av_queue_length)\n",
    "\n",
    "    tot_sojourn = sum(j.sojourn_time() for j in served_jobs)\n",
    "    av_sojourn_time = tot_sojourn/len(served_jobs)\n",
    "    print(\"Theoretical avg. sojourn time:\", (1./labda)*rho/(1-rho))\n",
    "    print(\"Simulated avg. sojourn time:\", av_sojourn_time)\n",
    "\n",
    "experiment_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical avg. queue length:  1.333333333333333\n",
      "Simulated avg. queue length: 0.7495049504950495\n",
      "Theoretical avg. sojourn time: 0.9999999999999999\n",
      "Simulated avg. sojourn time: 0.7389998666992809\n"
     ]
    }
   ],
   "source": [
    "def experiment_2a():\n",
    "    labda = 2.0\n",
    "    mu = 3.0\n",
    "    rho = labda / mu\n",
    "    F = expon(scale=1.0 / labda)  # interarrival time distributon\n",
    "    G = expon(scale=1.0 / mu)  # service time distributon\n",
    "    num_jobs = 1000\n",
    "\n",
    "    time = 0\n",
    "    for i in range(num_jobs):\n",
    "        time += F.rvs()\n",
    "        job = Job()\n",
    "        job.arrival_time = time\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)\n",
    "    av_queue_length = tot_queue / len(served_jobs)\n",
    "    print(\"Theoretical avg. queue length: \", rho * rho / (1 - rho))\n",
    "    print(\"Simulated avg. queue length:\", av_queue_length)\n",
    "\n",
    "    tot_sojourn = sum(j.sojourn_time() for j in served_jobs)\n",
    "    av_sojourn_time = tot_sojourn/len(served_jobs)\n",
    "    print(\"Theoretical avg. sojourn time:\", (1./labda)*rho/(1-rho))\n",
    "    print(\"Simulated avg. sojourn time:\", av_sojourn_time)\n",
    "\n",
    "experiment_2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.000002695509519, 2.0000027525679416, 5.000005448077461\n",
      "\n",
      "6.000007807537832, 2.000008753692769, 8.000016561230602\n",
      "\n",
      "9.000013148259669, 2.000000735900243, 11.000013884159912\n",
      "\n",
      "12.000022528752462, 2.000001265741311, 14.000023794493773\n",
      "\n",
      "15.000027027621853, 2.0000068305113157, 17.00003385813317\n",
      "\n",
      "18.000033368925727, 2.000003079168399, 20.000036448094125\n",
      "\n",
      "21.000039002721504, 2.0000059730566635, 23.000044975778167\n",
      "\n",
      "24.000039654048052, 2.0000087385505694, 26.00004839259862\n",
      "\n",
      "27.000043611195366, 2.000004346225087, 29.000047957420453\n",
      "\n",
      "30.00004829631633, 2.0000082152836574, 32.000056511599986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "stack = [] # this is the event stack\n",
    "queue = []\n",
    "served_jobs = [] # used for statistics\n",
    "\n",
    "def experiment_3():\n",
    "    labda = 2.0\n",
    "    mu = 3.0\n",
    "    rho = labda / mu\n",
    "    F = uniform(3, 0.00001)\n",
    "    G = uniform(2, 0.00001)\n",
    "    num_jobs = 10\n",
    "\n",
    "    time = 0\n",
    "    for i in range(num_jobs):\n",
    "        time += F.rvs()\n",
    "        job = Job()\n",
    "        job.arrival_time = time\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    for j in served_jobs:\n",
    "        print(j)\n",
    "\n",
    "\n",
    "experiment_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK:  0.722222222222222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pollaczek_khinchine(labda, G):\n",
    "    ES = G.mean()\n",
    "    rho = labda*ES\n",
    "    ce2 = G.var()/ES/ES\n",
    "    EW = (1.+ce2)/2 * rho/(1-rho)*ES\n",
    "    return EW\n",
    "\n",
    "\n",
    "labda = 1./3\n",
    "F = expon(scale=1./labda)  # interarrival time distributon\n",
    "G = uniform(1, 2)\n",
    "\n",
    "print(\"PK: \", labda*pollaczek_khinchine(labda,G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES:  2.5 rho:  0.8333333333333333\n",
      "Theoretical avg. queue length:  2.333333333333332\n",
      "Simulated avg. queue length: 1.63\n",
      "Theoretical avg. sojourn time:  9.499999999999996\n",
      "Avg. sojourn time: 7.575264285500065\n"
     ]
    }
   ],
   "source": [
    "stack = [] # this is the event stack\n",
    "queue = []\n",
    "served_jobs = [] # used for statistics\n",
    "\n",
    "def test_mg1():\n",
    "    job = Job()\n",
    "    labda = 1.0 / 3\n",
    "    F = expon(scale=1.0 / labda)  # interarrival time distributon\n",
    "    G = uniform(1, 3)\n",
    "    print(\"ES: \", G.mean(), \"rho: \", labda * G.mean())\n",
    "\n",
    "    num_jobs = 1000\n",
    "\n",
    "    time = 0\n",
    "    for i in range(num_jobs):\n",
    "        time += F.rvs()\n",
    "        job = Job()\n",
    "        job.arrival_time = time\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)\n",
    "    av_queue_length = tot_queue / len(served_jobs)\n",
    "    print(\"Theoretical avg. queue length: \", labda * pollaczek_khinchine(labda, G))\n",
    "    print(\"Simulated avg. queue length:\", av_queue_length)\n",
    "\n",
    "    tot_sojourn = sum(j.sojourn_time() for j in served_jobs)\n",
    "    av_sojourn_time = tot_sojourn / len(served_jobs)\n",
    "    print(\"Theoretical avg. sojourn time: \", pollaczek_khinchine(labda, G) + G.mean())\n",
    "    print(\"Avg. sojourn time:\", av_sojourn_time)\n",
    "\n",
    "test_mg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "F = np.sort(uniform(0, 120).rvs(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated avg. queue length: 5.0\n",
      "Avg. sojourn time: 17.437362698760733\n",
      "Queue length distributon, sloppy output\n",
      "[(0, 13), (1, 6), (2, 7), (3, 7), (4, 2), (5, 2), (6, 4), (7, 1), (8, 3), (9, 2), (10, 1), (11, 2), (12, 2), (13, 2), (14, 4), (15, 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "stack = [] # this is the event stack\n",
    "queue = []\n",
    "served_jobs = [] # used for statistics\n",
    "\n",
    "def check_in():\n",
    "    num_jobs = 60\n",
    "    F = np.sort(uniform(0, 120).rvs(num_jobs))\n",
    "    G = uniform(1, 3)\n",
    "\n",
    "    for i in range(num_jobs):\n",
    "        job = Job()\n",
    "        job.arrival_time = F[i]\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)\n",
    "    av_queue_length = tot_queue / len(served_jobs)\n",
    "    print(\"Simulated avg. queue length:\", av_queue_length)\n",
    "\n",
    "    tot_sojourn = sum(j.sojourn_time() for j in served_jobs)\n",
    "    av_sojourn_time = tot_sojourn / len(served_jobs)\n",
    "    print(\"Avg. sojourn time:\", av_sojourn_time)\n",
    "\n",
    "    c = Counter([j.queue_length_at_arrival for j in served_jobs])\n",
    "    print(\"Queue length distributon, sloppy output\")\n",
    "    print(sorted(c.items()))\n",
    "\n",
    "\n",
    "check_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [job.arrival_time for job in served_jobs]\n",
    "y = [job.queue_length_at_arrival for job in served_jobs]\n",
    "\n",
    "plt.plot(x, y, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated avg. queue length: 0.85\n",
      "Avg. sojourn time: 4.118316559733122\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stack = [] # this is the event stack\n",
    "queue = []\n",
    "served_jobs = [] # used for statistics\n",
    "\n",
    "def check_in_2():\n",
    "    num_jobs = 60\n",
    "    F = np.sort(uniform(0, 150).rvs(num_jobs))\n",
    "    G = uniform(1, 2)\n",
    "\n",
    "    for i in range(num_jobs):\n",
    "        job = Job()\n",
    "        job.arrival_time = F[i]\n",
    "        job.service_time = G.rvs()\n",
    "        heappush(stack, (job.arrival_time, job, ARRIVAL))\n",
    "\n",
    "    while stack:\n",
    "        time, job, typ = heappop(stack)\n",
    "        if typ == ARRIVAL:\n",
    "            handle_arrival(time, job)\n",
    "        else:\n",
    "            handle_departure(time, job)\n",
    "            served_jobs.append(job)\n",
    "\n",
    "    tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)\n",
    "    av_queue_length = tot_queue / len(served_jobs)\n",
    "    print(\"Simulated avg. queue length:\", av_queue_length)\n",
    "\n",
    "    tot_sojourn = sum(j.sojourn_time() for j in served_jobs)\n",
    "    av_sojourn_time = tot_sojourn / len(served_jobs)\n",
    "    print(\"Avg. sojourn time:\", av_sojourn_time)\n",
    "\n",
    "    x = [job.arrival_time for job in served_jobs]\n",
    "    y = [job.queue_length_at_arrival for job in served_jobs]\n",
    "    \n",
    "    plt.plot(x, y, \"o\")\n",
    "    plt.show()\n",
    "check_in_2()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
